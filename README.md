# ğŸ“¦ MeLi Classifier: Ecossistema de Engenharia de Dados & NLP

Uma plataforma completa de Engenharia de Dados para extraÃ§Ã£o (EL), transformaÃ§Ã£o (T) e anÃ¡lise de sentimentos em larga escala, utilizando arquitetura de microsserviÃ§os orquestrada.

## ğŸ§  O Que Este Projeto Faz? (Para Leigos e Especialistas)

Imagine que vocÃª quer saber se um produto no Mercado Livre Ã© bom ou ruim, mas ele tem milhares de comentÃ¡rios. Ler um por um Ã© impossÃ­vel.

O MeLi Classifier automatiza esse processo de forma inteligente e escalÃ¡vel:

1) Busca todos os comentÃ¡rios de um produto automaticamente.

2) LÃª e Interpreta cada comentÃ¡rio usando InteligÃªncia Artificial (nÃ£o apenas palavras-chave, mas entendendo o contexto e a ironia).

3) Classifica se o sentimento Ã© Positivo ou Negativo.

3) Gera um Dashboard com mÃ©tricas de confianÃ§a e exemplos prÃ¡ticos.

A grande diferenÃ§a? Isso nÃ£o roda tudo num Ãºnico script no meu computador. Ã‰ um sistema complexo onde cada tarefa Ã© feita por um "robÃ´" (container) diferente, todos coordenados por um "gerente" (Airflow) na nuvem.

## ğŸ—ï¸ A Arquitetura (Por Que Ã© Complexo?)

A maioria dos projetos de dados iniciantes sÃ£o "monÃ³litos": um Ãºnico arquivo Python que faz tudo. Se o scraping falhar, a anÃ¡lise para. Se o modelo de IA for pesado, o site trava.

Este projeto adota uma Arquitetura de Engenharia de Dados Profissional:

1. Desacoplamento Total (MicrosserviÃ§os)

Em vez de um programa gigante, temos 3 programas independentes:

    - O Dashboard (Frontend): Apenas mostra dados. NÃ£o processa nada pesado.

    - O Scraper (Worker 1): Especialista em ir Ã  web e buscar dados brutos.

    - O Analisador (Worker 2): Um computador dedicado apenas para rodar modelos pesados de IA (BERT).

2. OrquestraÃ§Ã£o (O "CÃ©rebro")

Usamos o Apache Airflow, a ferramenta padrÃ£o da indÃºstria para pipelines de dados. Ele decide quando ligar cada robÃ´, verifica se eles terminaram com sucesso e tenta novamente em caso de falha.

3. Data Lake (A "MemÃ³ria")

Nada fica salvo "na mÃ¡quina". Tudo vai para a nuvem (AWS S3). Isso garante que os dados persistam mesmo que os servidores sejam destruÃ­dos e recriados (o que acontece a cada execuÃ§Ã£o).

## ğŸš€ Tecnologias e DecisÃµes TÃ©cnicas

**ğŸ•·ï¸ ExtraÃ§Ã£o de Dados (Scraper)**

- Desafio: O Mercado Livre tem estruturas HTML complexas e dinÃ¢micas.

- SoluÃ§Ã£o: Script Python resiliente que navega por paginaÃ§Ã£o, trata erros de rede e normaliza os dados brutos antes de enviar para o Data Lake.

**ğŸ§  InteligÃªncia Artificial (Analyzer)**

- Desafio: AnÃ¡lise de sentimento tradicional (baseada em palavras positivas/negativas) falha em portuguÃªs (ex: "NÃ£o gostei nada" tem a palavra "gostei").

- SoluÃ§Ã£o: Uso do BERTimbau, um modelo Transformer (estado da arte em NLP) prÃ©-treinado especificamente em portuguÃªs brasileiro pela NeuralMind. Ele entende contexto, negaÃ§Ã£o e sarcasmo.

**âš™ï¸ Engenharia (Docker & Airflow)**

- Docker-in-Docker: Uma tÃ©cnica avanÃ§ada onde o Airflow nÃ£o executa o cÃ³digo Python diretamente. Ele usa o DockerOperator para criar, executar e destruir containers inteiros para cada tarefa. Isso garante isolamento total de dependÃªncias (o Scraper nÃ£o precisa ter o PyTorch instalado, e o Analyzer nÃ£o precisa de bibliotecas web).


## ğŸ“‚ Estrutura do RepositÃ³rio

- **airflow/:** ContÃ©m as DAGs (os "mapas" do fluxo de trabalho) que dizem ao Airflow o que fazer.

- **analyzer-app/:** O cÃ³digo do modelo de IA. Possui seu prÃ³prio Dockerfile e requirements.txt (pesado, com PyTorch).

- **scraper-app/:** O cÃ³digo de coleta de dados. Possui seu prÃ³prio Dockerfile (leve).

- **streamlit_app/:**  O site em Streamlit. Possui lÃ³gica de "polling" para verificar no S3 se os dados jÃ¡ estÃ£o prontos.

- **docker-compose.yml:** O maestro que sobe toda a infraestrutura localmente (Banco de dados, Webserver, Scheduler e Frontend).


## ğŸ”„ Fluxo de Dados

- **Trigger:** usuÃ¡rio insere o ID do produto.
- **VerificaÃ§Ã£o:** o sistema checa no S3 se o resultado jÃ¡ existe.
- **OrquestraÃ§Ã£o:** se nÃ£o existir, o Streamlit aciona o Airflow.
- **ExtraÃ§Ã£o (EL):** o *scraper-app* coleta dados e salva no Raw.
- **TransformaÃ§Ã£o (T):** o *analyzer-app* aplica BERT e grava o JSON final.
- **VisualizaÃ§Ã£o:** o Dashboard lÃª o arquivo e exibe mÃ©tricas e grÃ¡ficos.

## ğŸ› ï¸ Tech Stack

### Infraestrutura & OrquestraÃ§Ã£o
- Docker + Docker Compose  
- Apache Airflow  
- AWS S3 (Data Lake)

### MicrosserviÃ§os
- **Scraper:** Python, Requests, BS4  
- **Analyzer:** Python, PyTorch, Transformers (BERTimbau)  
- **Dashboard:** Streamlit

## ğŸš€ Como Executar Localmente

### PrÃ©-requisitos
- Docker Desktop instalado e rodando + Docker Compose  
- Bucket S3 configurado  
- Credenciais AWS exportadas no ambiente com permissÃ£o de leitura/escrita em um bucket S3.


### Passo 1 â€” Clonar o repositÃ³rio
```bash
git clone https://github.com/biancamayor/meli_classifier.git
cd meli_classifier
```

### Passo 2 â€” Exportar credenciais

**Linux/Mac**
```bash
export AWS_ACCESS_KEY_ID=sua_chave
export AWS_SECRET_ACCESS_KEY=sua_senha
```

**Windows (PowerShell)**
```ps1
$env:AWS_ACCESS_KEY_ID="sua_chave"
$env:AWS_SECRET_ACCESS_KEY="sua_senha"
```

### Passo 3 â€” Construa os Workers: Precisamos criar as imagens dos "robÃ´s" que o Airflow vai usar
```bash
cd scraper-app && docker build -t scraper-app:latest . && cd ..
cd analyzer-app && docker build -t analyzer-app:latest . && cd ..
```

### Passo 4 â€” Subir o ambiente
```bash
docker-compose up --build
```

### Passo 5 â€” Configurar Airflow
- Acesse **http://localhost:8081**  
  Login: `seulogin` / Senha: `suasenha`
- VÃ¡ em *Admin â†’ Variables*  
- Adicione:
  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`
- Ative a DAG `meli_orchestrator`

### Passo 6 â€” Usar o Streamlit
Acesse: **http://localhost:8501**

Insira um ID de produto (ex.: `MLB-12345678`) e acompanhe o pipeline rodar.

## ğŸ“‚ Estrutura do Projeto

```
â”œâ”€â”€ airflow/               
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ orchestrator.py
â”œâ”€â”€ local-airflow/
|   â””â”€â”€ Dockerfile.airflow
|   â””â”€â”€ requirements.txt         
â”œâ”€â”€ analyzer-app/          
â”‚   â”œâ”€â”€ src/
|   |   â””â”€â”€ bertimbau_classifier.py
â”‚   â”œâ”€â”€ analyze.py
â”‚   â””â”€â”€ Dockerfile
|   â””â”€â”€ requirements.txt
â”œâ”€â”€ scraper-app/           
â”‚   â”œâ”€â”€ scraper.py
â”‚   â””â”€â”€ Dockerfile
|   â””â”€â”€ requirements.txt
â”œâ”€â”€ streamlit_app/         
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
|   â””â”€â”€ requirements.txt
â””â”€â”€ docker-compose.yml
```

## âœ¨ Destaques e Aprendizados

- **Docker-in-Docker:** Airflow executa containers isolados via DockerOperator.  
- **Gerenciamento de Estado:** Streamlit com polling inteligente.  
- **ResiliÃªncia:** tratamento robusto de erros de rede, API e permissÃµes AWS.

## ğŸ“ Contato

Desenvolvido por **Bianca**  
ğŸ’¼ LinkedIn: https://www.linkedin.com/in/bianca-mayor 